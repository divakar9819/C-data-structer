{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "shallow_net_in_tensorflow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNidikpOEhw2Z/diD1Vtlgq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divakar9819/C-data-structer/blob/master/shallow_net_in_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-9BNarpXm2q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "9852365e-f497-4526-b298-7cc18e728611"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlYnbRSoX0mm",
        "colab_type": "code",
        "outputId": "46371773-4df8-4a8f-961c-cf97ff440bb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "!pip freeze | grep tensorflow"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mesh-tensorflow==0.1.9\n",
            "tensorflow==1.15.0\n",
            "tensorflow-datasets==2.0.0\n",
            "tensorflow-estimator==1.15.1\n",
            "tensorflow-gan==2.0.0\n",
            "tensorflow-hub==0.7.0\n",
            "tensorflow-metadata==0.21.1\n",
            "tensorflow-privacy==0.2.2\n",
            "tensorflow-probability==0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-gE7mpeYJJV",
        "colab_type": "code",
        "outputId": "fc78a8c1-67e2-4add-ed28-ea6aa16a2b93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pip install tensorflow==2.0.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 120kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 47.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.9.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.1.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 50.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.34.2)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.27.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.17.5)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.7.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.21.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.2.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (45.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_core",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7yjb9JVYP8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lKfSFrsYoje",
        "colab_type": "code",
        "outputId": "9b2990e0-a482-4164-8737-b80a6485d062",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "(X_train, y_train), (X_valid, y_valid) = mnist.load_data()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSQZSt3vYuO4",
        "colab_type": "code",
        "outputId": "6a7e4422-9ed5-464b-cdfa-795fd97d9c12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWONzpTHY4DD",
        "colab_type": "code",
        "outputId": "0411581a-eb0b-4c60-bce3-ffc970cc3d16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbep0YUacWT0",
        "colab_type": "code",
        "outputId": "36f9bf31-036c-44ff-88e5-bd4c3cd6813e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[0:15]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2TjlauDcpOr",
        "colab_type": "code",
        "outputId": "fff18aba-92c3-45cd-dbd1-2e8dc0f0ede7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        }
      },
      "source": [
        "plt.figure(figsize=(5,5))\n",
        "for k in range(12):\n",
        "    plt.subplot(3, 4, k+1)\n",
        "    plt.imshow(X_train[k], cmap='Greys')\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAE2CAYAAABftkimAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngEhogAEgEhpgAIjk/wHtHjTpBcr+dgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 360x360 with 12 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnBXz5-lf5P6",
        "colab_type": "code",
        "outputId": "22452939-1728-43d3-9812-bcc54e843755",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_valid.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBAoZZwvf9tI",
        "colab_type": "code",
        "outputId": "5512f565-f5ea-4498-e45e-1918dc6ea418",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_valid.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enTaE7IGgAdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf6ZlAuHgXDy",
        "colab_type": "code",
        "outputId": "0c1784db-652f-48e9-f3b9-e87e6721c243",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.imshow(X_valid[0], cmap='Greys')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f6beadf53c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkJgrVgOgZK9",
        "colab_type": "code",
        "outputId": "dd3dd36f-f05f-4e05-acb0-f5a7c28e3279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X_valid[0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  84, 185, 159, 151,  60,  36,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0, 222, 254, 254, 254, 254, 241, 198,\n",
              "        198, 198, 198, 198, 198, 198, 198, 170,  52,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  67, 114,  72, 114, 163, 227, 254,\n",
              "        225, 254, 254, 254, 250, 229, 254, 254, 140,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  17,  66,\n",
              "         14,  67,  67,  67,  59,  21, 236, 254, 106,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,  83, 253, 209,  18,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,  22, 233, 255,  83,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 129, 254, 238,  44,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,  59, 249, 254,  62,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0, 133, 254, 187,   5,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   9, 205, 248,  58,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0, 126, 254, 182,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  75, 251, 240,  57,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         19, 221, 254, 166,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "        203, 254, 219,  35,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38,\n",
              "        254, 254,  77,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  31, 224,\n",
              "        254, 115,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 133, 254,\n",
              "        254,  52,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  61, 242, 254,\n",
              "        254,  52,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 121, 254, 254,\n",
              "        219,  40,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 121, 254, 207,\n",
              "         18,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhIuiiv-gxPz",
        "colab_type": "code",
        "outputId": "e64b32af-7bfd-4b5a-81a9-617274e24b1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_valid[0]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QacZw9DZg0mS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkLuUL-LiXW7",
        "colab_type": "text"
      },
      "source": [
        "Processing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKiR3rb9iS-T",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdziXpr3iFmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(60000, 784).astype('float32')\n",
        "X_valid = X_valid.reshape(10000, 784).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF_yayMLiitm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train /= 255\n",
        "X_valid /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYTKRcA4in-g",
        "colab_type": "code",
        "outputId": "d7e9ef7d-5322-4071-f7a8-00a0d958df64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X_valid[0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.32941177, 0.7254902 , 0.62352943,\n",
              "       0.5921569 , 0.23529412, 0.14117648, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.87058824, 0.99607843, 0.99607843, 0.99607843, 0.99607843,\n",
              "       0.94509804, 0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 ,\n",
              "       0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 , 0.6666667 ,\n",
              "       0.20392157, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.2627451 , 0.44705883,\n",
              "       0.28235295, 0.44705883, 0.6392157 , 0.8901961 , 0.99607843,\n",
              "       0.88235295, 0.99607843, 0.99607843, 0.99607843, 0.98039216,\n",
              "       0.8980392 , 0.99607843, 0.99607843, 0.54901963, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.06666667, 0.25882354, 0.05490196, 0.2627451 ,\n",
              "       0.2627451 , 0.2627451 , 0.23137255, 0.08235294, 0.9254902 ,\n",
              "       0.99607843, 0.41568628, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.3254902 , 0.99215686, 0.81960785, 0.07058824,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.08627451, 0.9137255 ,\n",
              "       1.        , 0.3254902 , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.5058824 , 0.99607843, 0.93333334, 0.17254902,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.23137255, 0.9764706 ,\n",
              "       0.99607843, 0.24313726, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.52156866, 0.99607843, 0.73333335, 0.01960784,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.03529412, 0.8039216 ,\n",
              "       0.972549  , 0.22745098, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.49411765, 0.99607843, 0.7137255 , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.29411766, 0.9843137 ,\n",
              "       0.9411765 , 0.22352941, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.07450981, 0.8666667 , 0.99607843, 0.6509804 , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.01176471, 0.79607844, 0.99607843,\n",
              "       0.85882354, 0.13725491, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.14901961, 0.99607843, 0.99607843, 0.3019608 , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.12156863, 0.8784314 , 0.99607843,\n",
              "       0.4509804 , 0.00392157, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.52156866, 0.99607843, 0.99607843, 0.20392157, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.23921569, 0.9490196 , 0.99607843,\n",
              "       0.99607843, 0.20392157, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.4745098 , 0.99607843, 0.99607843, 0.85882354, 0.15686275,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.4745098 , 0.99607843,\n",
              "       0.8117647 , 0.07058824, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB9y2g9emcx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_classes = 10\n",
        "y_train = to_categorical(y_train, n_classes)\n",
        "y_valid = to_categorical(y_valid, n_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JsJcvJ4nFCd",
        "colab_type": "code",
        "outputId": "8e0b70a7-1062-48e2-e9a1-156c238e8b76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X_valid[0]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.32941177, 0.7254902 , 0.62352943,\n",
              "       0.5921569 , 0.23529412, 0.14117648, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.87058824, 0.99607843, 0.99607843, 0.99607843, 0.99607843,\n",
              "       0.94509804, 0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 ,\n",
              "       0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 , 0.6666667 ,\n",
              "       0.20392157, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.2627451 , 0.44705883,\n",
              "       0.28235295, 0.44705883, 0.6392157 , 0.8901961 , 0.99607843,\n",
              "       0.88235295, 0.99607843, 0.99607843, 0.99607843, 0.98039216,\n",
              "       0.8980392 , 0.99607843, 0.99607843, 0.54901963, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.06666667, 0.25882354, 0.05490196, 0.2627451 ,\n",
              "       0.2627451 , 0.2627451 , 0.23137255, 0.08235294, 0.9254902 ,\n",
              "       0.99607843, 0.41568628, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.3254902 , 0.99215686, 0.81960785, 0.07058824,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.08627451, 0.9137255 ,\n",
              "       1.        , 0.3254902 , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.5058824 , 0.99607843, 0.93333334, 0.17254902,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.23137255, 0.9764706 ,\n",
              "       0.99607843, 0.24313726, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.52156866, 0.99607843, 0.73333335, 0.01960784,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.03529412, 0.8039216 ,\n",
              "       0.972549  , 0.22745098, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.49411765, 0.99607843, 0.7137255 , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.29411766, 0.9843137 ,\n",
              "       0.9411765 , 0.22352941, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.07450981, 0.8666667 , 0.99607843, 0.6509804 , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.01176471, 0.79607844, 0.99607843,\n",
              "       0.85882354, 0.13725491, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.14901961, 0.99607843, 0.99607843, 0.3019608 , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.12156863, 0.8784314 , 0.99607843,\n",
              "       0.4509804 , 0.00392157, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.52156866, 0.99607843, 0.99607843, 0.20392157, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.23921569, 0.9490196 , 0.99607843,\n",
              "       0.99607843, 0.20392157, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.4745098 , 0.99607843, 0.99607843, 0.85882354, 0.15686275,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.4745098 , 0.99607843,\n",
              "       0.8117647 , 0.07058824, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLTA2KRgn3j2",
        "colab_type": "code",
        "outputId": "a9ffe8e4-2975-490d-bc69-4344e9d58574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_valid[0]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw6r0Rz1n70L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "2d4b955d-572b-43ea-a1d1-e503974ae772"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(64, activation='sigmoid', input_shape=(784,)))\n",
        "model.add(Dense(10, activation='softmax'))\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEEKYkeOoQI9",
        "colab_type": "code",
        "outputId": "c394dba4-748a-450e-df04-a325aba92f40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                50240     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 50,890\n",
            "Trainable params: 50,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ksHuVhCodmr",
        "colab_type": "code",
        "outputId": "f4f9c8e2-7f63-4925-9e1a-e34092ddac1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(64*784)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50176"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQVb5JeeosLd",
        "colab_type": "code",
        "outputId": "a429c902-d3cd-4f9b-81b7-f2791eaaaa56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(64*784)+64"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50240"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy1bML15ot5l",
        "colab_type": "code",
        "outputId": "b766f54c-0da8-49ba-bbbc-a36b6e1fe6b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(64*784)+10"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50186"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOlD4BI3oy5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ9gV9HNo2sn",
        "colab_type": "text"
      },
      "source": [
        "## Configure **model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chyanJPIo7CJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gbtE_foq4ZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta8zO2msq5Ja",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRlOsPdSq7jQ",
        "colab_type": "code",
        "outputId": "e4ccb840-76be-49fc-efa1-bad931e368fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=128, epochs=200, verbose=1, validation_data=(X_valid, y_valid))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/200\n",
            "60000/60000 [==============================] - 3s 45us/sample - loss: 0.0935 - acc: 0.1045 - val_loss: 0.0927 - val_acc: 0.1057\n",
            "Epoch 2/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0922 - acc: 0.1109 - val_loss: 0.0918 - val_acc: 0.1164\n",
            "Epoch 3/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0914 - acc: 0.1244 - val_loss: 0.0911 - val_acc: 0.1310\n",
            "Epoch 4/200\n",
            "60000/60000 [==============================] - 1s 23us/sample - loss: 0.0908 - acc: 0.1392 - val_loss: 0.0906 - val_acc: 0.1482\n",
            "Epoch 5/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0904 - acc: 0.1545 - val_loss: 0.0901 - val_acc: 0.1628\n",
            "Epoch 6/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0900 - acc: 0.1704 - val_loss: 0.0897 - val_acc: 0.1809\n",
            "Epoch 7/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0896 - acc: 0.1877 - val_loss: 0.0894 - val_acc: 0.2017\n",
            "Epoch 8/200\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 0.0893 - acc: 0.2104 - val_loss: 0.0890 - val_acc: 0.2313\n",
            "Epoch 9/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0889 - acc: 0.2377 - val_loss: 0.0887 - val_acc: 0.2598\n",
            "Epoch 10/200\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 0.0886 - acc: 0.2666 - val_loss: 0.0884 - val_acc: 0.2899\n",
            "Epoch 11/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0883 - acc: 0.2966 - val_loss: 0.0881 - val_acc: 0.3173\n",
            "Epoch 12/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0880 - acc: 0.3215 - val_loss: 0.0878 - val_acc: 0.3425\n",
            "Epoch 13/200\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 0.0877 - acc: 0.3446 - val_loss: 0.0875 - val_acc: 0.3635\n",
            "Epoch 14/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0875 - acc: 0.3639 - val_loss: 0.0872 - val_acc: 0.3829\n",
            "Epoch 15/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0872 - acc: 0.3818 - val_loss: 0.0869 - val_acc: 0.4018\n",
            "Epoch 16/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0869 - acc: 0.3958 - val_loss: 0.0866 - val_acc: 0.4160\n",
            "Epoch 17/200\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 0.0866 - acc: 0.4099 - val_loss: 0.0863 - val_acc: 0.4263\n",
            "Epoch 18/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0862 - acc: 0.4215 - val_loss: 0.0860 - val_acc: 0.4382\n",
            "Epoch 19/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0859 - acc: 0.4299 - val_loss: 0.0856 - val_acc: 0.4475\n",
            "Epoch 20/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0856 - acc: 0.4386 - val_loss: 0.0853 - val_acc: 0.4552\n",
            "Epoch 21/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0853 - acc: 0.4454 - val_loss: 0.0850 - val_acc: 0.4609\n",
            "Epoch 22/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0849 - acc: 0.4519 - val_loss: 0.0846 - val_acc: 0.4648\n",
            "Epoch 23/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0846 - acc: 0.4550 - val_loss: 0.0843 - val_acc: 0.4682\n",
            "Epoch 24/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0843 - acc: 0.4578 - val_loss: 0.0839 - val_acc: 0.4711\n",
            "Epoch 25/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0839 - acc: 0.4622 - val_loss: 0.0835 - val_acc: 0.4724\n",
            "Epoch 26/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0835 - acc: 0.4648 - val_loss: 0.0832 - val_acc: 0.4741\n",
            "Epoch 27/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0832 - acc: 0.4658 - val_loss: 0.0828 - val_acc: 0.4760\n",
            "Epoch 28/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0828 - acc: 0.4678 - val_loss: 0.0824 - val_acc: 0.4775\n",
            "Epoch 29/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0824 - acc: 0.4689 - val_loss: 0.0820 - val_acc: 0.4784\n",
            "Epoch 30/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0820 - acc: 0.4704 - val_loss: 0.0816 - val_acc: 0.4808\n",
            "Epoch 31/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0816 - acc: 0.4718 - val_loss: 0.0811 - val_acc: 0.4823\n",
            "Epoch 32/200\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 0.0812 - acc: 0.4739 - val_loss: 0.0807 - val_acc: 0.4847\n",
            "Epoch 33/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0807 - acc: 0.4766 - val_loss: 0.0803 - val_acc: 0.4862\n",
            "Epoch 34/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0803 - acc: 0.4782 - val_loss: 0.0798 - val_acc: 0.4891\n",
            "Epoch 35/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0799 - acc: 0.4804 - val_loss: 0.0794 - val_acc: 0.4921\n",
            "Epoch 36/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0794 - acc: 0.4830 - val_loss: 0.0789 - val_acc: 0.4953\n",
            "Epoch 37/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0789 - acc: 0.4864 - val_loss: 0.0785 - val_acc: 0.4976\n",
            "Epoch 38/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0785 - acc: 0.4888 - val_loss: 0.0780 - val_acc: 0.5006\n",
            "Epoch 39/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0780 - acc: 0.4915 - val_loss: 0.0775 - val_acc: 0.5036\n",
            "Epoch 40/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0775 - acc: 0.4945 - val_loss: 0.0770 - val_acc: 0.5079\n",
            "Epoch 41/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0771 - acc: 0.4973 - val_loss: 0.0765 - val_acc: 0.5104\n",
            "Epoch 42/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0766 - acc: 0.5011 - val_loss: 0.0760 - val_acc: 0.5130\n",
            "Epoch 43/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0761 - acc: 0.5044 - val_loss: 0.0755 - val_acc: 0.5162\n",
            "Epoch 44/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0756 - acc: 0.5086 - val_loss: 0.0750 - val_acc: 0.5196\n",
            "Epoch 45/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0751 - acc: 0.5124 - val_loss: 0.0745 - val_acc: 0.5224\n",
            "Epoch 46/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0746 - acc: 0.5152 - val_loss: 0.0740 - val_acc: 0.5273\n",
            "Epoch 47/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0741 - acc: 0.5192 - val_loss: 0.0735 - val_acc: 0.5309\n",
            "Epoch 48/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0736 - acc: 0.5225 - val_loss: 0.0729 - val_acc: 0.5340\n",
            "Epoch 49/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0730 - acc: 0.5264 - val_loss: 0.0724 - val_acc: 0.5386\n",
            "Epoch 50/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0725 - acc: 0.5306 - val_loss: 0.0719 - val_acc: 0.5431\n",
            "Epoch 51/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0720 - acc: 0.5350 - val_loss: 0.0714 - val_acc: 0.5461\n",
            "Epoch 52/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0715 - acc: 0.5386 - val_loss: 0.0708 - val_acc: 0.5513\n",
            "Epoch 53/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0710 - acc: 0.5422 - val_loss: 0.0703 - val_acc: 0.5551\n",
            "Epoch 54/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0704 - acc: 0.5472 - val_loss: 0.0698 - val_acc: 0.5594\n",
            "Epoch 55/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0699 - acc: 0.5514 - val_loss: 0.0692 - val_acc: 0.5627\n",
            "Epoch 56/200\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 0.0694 - acc: 0.5554 - val_loss: 0.0687 - val_acc: 0.5684\n",
            "Epoch 57/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0689 - acc: 0.5594 - val_loss: 0.0682 - val_acc: 0.5729\n",
            "Epoch 58/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0684 - acc: 0.5639 - val_loss: 0.0676 - val_acc: 0.5770\n",
            "Epoch 59/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0678 - acc: 0.5687 - val_loss: 0.0671 - val_acc: 0.5811\n",
            "Epoch 60/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0673 - acc: 0.5729 - val_loss: 0.0666 - val_acc: 0.5850\n",
            "Epoch 61/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0668 - acc: 0.5767 - val_loss: 0.0661 - val_acc: 0.5887\n",
            "Epoch 62/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0663 - acc: 0.5817 - val_loss: 0.0655 - val_acc: 0.5944\n",
            "Epoch 63/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0658 - acc: 0.5865 - val_loss: 0.0650 - val_acc: 0.5993\n",
            "Epoch 64/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0653 - acc: 0.5912 - val_loss: 0.0645 - val_acc: 0.6040\n",
            "Epoch 65/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0648 - acc: 0.5956 - val_loss: 0.0640 - val_acc: 0.6081\n",
            "Epoch 66/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0642 - acc: 0.5995 - val_loss: 0.0635 - val_acc: 0.6126\n",
            "Epoch 67/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0637 - acc: 0.6040 - val_loss: 0.0630 - val_acc: 0.6172\n",
            "Epoch 68/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0633 - acc: 0.6087 - val_loss: 0.0625 - val_acc: 0.6210\n",
            "Epoch 69/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0628 - acc: 0.6130 - val_loss: 0.0620 - val_acc: 0.6255\n",
            "Epoch 70/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0623 - acc: 0.6170 - val_loss: 0.0615 - val_acc: 0.6304\n",
            "Epoch 71/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0618 - acc: 0.6212 - val_loss: 0.0610 - val_acc: 0.6366\n",
            "Epoch 72/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0613 - acc: 0.6256 - val_loss: 0.0605 - val_acc: 0.6419\n",
            "Epoch 73/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0608 - acc: 0.6308 - val_loss: 0.0600 - val_acc: 0.6453\n",
            "Epoch 74/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0604 - acc: 0.6356 - val_loss: 0.0595 - val_acc: 0.6499\n",
            "Epoch 75/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0599 - acc: 0.6402 - val_loss: 0.0591 - val_acc: 0.6529\n",
            "Epoch 76/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0594 - acc: 0.6449 - val_loss: 0.0586 - val_acc: 0.6582\n",
            "Epoch 77/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0590 - acc: 0.6490 - val_loss: 0.0582 - val_acc: 0.6632\n",
            "Epoch 78/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0585 - acc: 0.6528 - val_loss: 0.0577 - val_acc: 0.6671\n",
            "Epoch 79/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0581 - acc: 0.6572 - val_loss: 0.0572 - val_acc: 0.6706\n",
            "Epoch 80/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0577 - acc: 0.6610 - val_loss: 0.0568 - val_acc: 0.6749\n",
            "Epoch 81/200\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 0.0572 - acc: 0.6651 - val_loss: 0.0564 - val_acc: 0.6782\n",
            "Epoch 82/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0568 - acc: 0.6690 - val_loss: 0.0559 - val_acc: 0.6823\n",
            "Epoch 83/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0564 - acc: 0.6726 - val_loss: 0.0555 - val_acc: 0.6857\n",
            "Epoch 84/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0559 - acc: 0.6766 - val_loss: 0.0551 - val_acc: 0.6884\n",
            "Epoch 85/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0555 - acc: 0.6796 - val_loss: 0.0547 - val_acc: 0.6921\n",
            "Epoch 86/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0551 - acc: 0.6836 - val_loss: 0.0542 - val_acc: 0.6959\n",
            "Epoch 87/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0547 - acc: 0.6868 - val_loss: 0.0538 - val_acc: 0.6996\n",
            "Epoch 88/200\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0543 - acc: 0.6905 - val_loss: 0.0534 - val_acc: 0.7032\n",
            "Epoch 89/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0539 - acc: 0.6937 - val_loss: 0.0530 - val_acc: 0.7054\n",
            "Epoch 90/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0535 - acc: 0.6968 - val_loss: 0.0526 - val_acc: 0.7101\n",
            "Epoch 91/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0532 - acc: 0.6996 - val_loss: 0.0523 - val_acc: 0.7137\n",
            "Epoch 92/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0528 - acc: 0.7030 - val_loss: 0.0519 - val_acc: 0.7165\n",
            "Epoch 93/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0524 - acc: 0.7055 - val_loss: 0.0515 - val_acc: 0.7194\n",
            "Epoch 94/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0520 - acc: 0.7087 - val_loss: 0.0511 - val_acc: 0.7219\n",
            "Epoch 95/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0517 - acc: 0.7111 - val_loss: 0.0508 - val_acc: 0.7246\n",
            "Epoch 96/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0513 - acc: 0.7136 - val_loss: 0.0504 - val_acc: 0.7273\n",
            "Epoch 97/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0509 - acc: 0.7160 - val_loss: 0.0500 - val_acc: 0.7295\n",
            "Epoch 98/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0506 - acc: 0.7180 - val_loss: 0.0497 - val_acc: 0.7322\n",
            "Epoch 99/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0502 - acc: 0.7207 - val_loss: 0.0493 - val_acc: 0.7340\n",
            "Epoch 100/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0499 - acc: 0.7225 - val_loss: 0.0490 - val_acc: 0.7359\n",
            "Epoch 101/200\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 0.0496 - acc: 0.7248 - val_loss: 0.0486 - val_acc: 0.7387\n",
            "Epoch 102/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0492 - acc: 0.7266 - val_loss: 0.0483 - val_acc: 0.7416\n",
            "Epoch 103/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0489 - acc: 0.7286 - val_loss: 0.0480 - val_acc: 0.7441\n",
            "Epoch 104/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0486 - acc: 0.7305 - val_loss: 0.0476 - val_acc: 0.7450\n",
            "Epoch 105/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0482 - acc: 0.7322 - val_loss: 0.0473 - val_acc: 0.7469\n",
            "Epoch 106/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0479 - acc: 0.7339 - val_loss: 0.0470 - val_acc: 0.7482\n",
            "Epoch 107/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0476 - acc: 0.7358 - val_loss: 0.0467 - val_acc: 0.7496\n",
            "Epoch 108/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0473 - acc: 0.7378 - val_loss: 0.0464 - val_acc: 0.7527\n",
            "Epoch 109/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0470 - acc: 0.7391 - val_loss: 0.0461 - val_acc: 0.7542\n",
            "Epoch 110/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0467 - acc: 0.7408 - val_loss: 0.0458 - val_acc: 0.7558\n",
            "Epoch 111/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0464 - acc: 0.7426 - val_loss: 0.0454 - val_acc: 0.7566\n",
            "Epoch 112/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0461 - acc: 0.7444 - val_loss: 0.0451 - val_acc: 0.7585\n",
            "Epoch 113/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0458 - acc: 0.7461 - val_loss: 0.0449 - val_acc: 0.7592\n",
            "Epoch 114/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0455 - acc: 0.7475 - val_loss: 0.0446 - val_acc: 0.7605\n",
            "Epoch 115/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0452 - acc: 0.7488 - val_loss: 0.0443 - val_acc: 0.7624\n",
            "Epoch 116/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0449 - acc: 0.7500 - val_loss: 0.0440 - val_acc: 0.7642\n",
            "Epoch 117/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0447 - acc: 0.7516 - val_loss: 0.0437 - val_acc: 0.7659\n",
            "Epoch 118/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0444 - acc: 0.7533 - val_loss: 0.0434 - val_acc: 0.7674\n",
            "Epoch 119/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0441 - acc: 0.7549 - val_loss: 0.0432 - val_acc: 0.7687\n",
            "Epoch 120/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0438 - acc: 0.7565 - val_loss: 0.0429 - val_acc: 0.7700\n",
            "Epoch 121/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0436 - acc: 0.7579 - val_loss: 0.0426 - val_acc: 0.7716\n",
            "Epoch 122/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0433 - acc: 0.7599 - val_loss: 0.0423 - val_acc: 0.7725\n",
            "Epoch 123/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0430 - acc: 0.7614 - val_loss: 0.0421 - val_acc: 0.7743\n",
            "Epoch 124/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0428 - acc: 0.7631 - val_loss: 0.0418 - val_acc: 0.7759\n",
            "Epoch 125/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0425 - acc: 0.7648 - val_loss: 0.0416 - val_acc: 0.7770\n",
            "Epoch 126/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0423 - acc: 0.7664 - val_loss: 0.0413 - val_acc: 0.7788\n",
            "Epoch 127/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0420 - acc: 0.7681 - val_loss: 0.0410 - val_acc: 0.7806\n",
            "Epoch 128/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0418 - acc: 0.7700 - val_loss: 0.0408 - val_acc: 0.7825\n",
            "Epoch 129/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0415 - acc: 0.7718 - val_loss: 0.0406 - val_acc: 0.7850\n",
            "Epoch 130/200\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 0.0413 - acc: 0.7740 - val_loss: 0.0403 - val_acc: 0.7866\n",
            "Epoch 131/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0410 - acc: 0.7757 - val_loss: 0.0401 - val_acc: 0.7885\n",
            "Epoch 132/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0408 - acc: 0.7774 - val_loss: 0.0398 - val_acc: 0.7905\n",
            "Epoch 133/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0406 - acc: 0.7793 - val_loss: 0.0396 - val_acc: 0.7922\n",
            "Epoch 134/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0403 - acc: 0.7814 - val_loss: 0.0394 - val_acc: 0.7944\n",
            "Epoch 135/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0401 - acc: 0.7830 - val_loss: 0.0391 - val_acc: 0.7957\n",
            "Epoch 136/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0399 - acc: 0.7846 - val_loss: 0.0389 - val_acc: 0.7980\n",
            "Epoch 137/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0396 - acc: 0.7864 - val_loss: 0.0387 - val_acc: 0.7996\n",
            "Epoch 138/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0394 - acc: 0.7879 - val_loss: 0.0384 - val_acc: 0.8008\n",
            "Epoch 139/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0392 - acc: 0.7893 - val_loss: 0.0382 - val_acc: 0.8024\n",
            "Epoch 140/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0390 - acc: 0.7912 - val_loss: 0.0380 - val_acc: 0.8036\n",
            "Epoch 141/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0388 - acc: 0.7927 - val_loss: 0.0378 - val_acc: 0.8046\n",
            "Epoch 142/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0385 - acc: 0.7944 - val_loss: 0.0376 - val_acc: 0.8064\n",
            "Epoch 143/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0383 - acc: 0.7963 - val_loss: 0.0374 - val_acc: 0.8078\n",
            "Epoch 144/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0381 - acc: 0.7978 - val_loss: 0.0371 - val_acc: 0.8099\n",
            "Epoch 145/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0379 - acc: 0.7993 - val_loss: 0.0369 - val_acc: 0.8114\n",
            "Epoch 146/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0377 - acc: 0.8012 - val_loss: 0.0367 - val_acc: 0.8133\n",
            "Epoch 147/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0375 - acc: 0.8029 - val_loss: 0.0365 - val_acc: 0.8148\n",
            "Epoch 148/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0373 - acc: 0.8044 - val_loss: 0.0363 - val_acc: 0.8164\n",
            "Epoch 149/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0371 - acc: 0.8059 - val_loss: 0.0361 - val_acc: 0.8175\n",
            "Epoch 150/200\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 0.0369 - acc: 0.8070 - val_loss: 0.0359 - val_acc: 0.8182\n",
            "Epoch 151/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0367 - acc: 0.8089 - val_loss: 0.0357 - val_acc: 0.8197\n",
            "Epoch 152/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0365 - acc: 0.8105 - val_loss: 0.0355 - val_acc: 0.8222\n",
            "Epoch 153/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0363 - acc: 0.8117 - val_loss: 0.0353 - val_acc: 0.8237\n",
            "Epoch 154/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0361 - acc: 0.8130 - val_loss: 0.0351 - val_acc: 0.8248\n",
            "Epoch 155/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0359 - acc: 0.8141 - val_loss: 0.0350 - val_acc: 0.8260\n",
            "Epoch 156/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0358 - acc: 0.8157 - val_loss: 0.0348 - val_acc: 0.8271\n",
            "Epoch 157/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0356 - acc: 0.8169 - val_loss: 0.0346 - val_acc: 0.8280\n",
            "Epoch 158/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0354 - acc: 0.8181 - val_loss: 0.0344 - val_acc: 0.8288\n",
            "Epoch 159/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0352 - acc: 0.8196 - val_loss: 0.0342 - val_acc: 0.8294\n",
            "Epoch 160/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0350 - acc: 0.8206 - val_loss: 0.0340 - val_acc: 0.8301\n",
            "Epoch 161/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0349 - acc: 0.8218 - val_loss: 0.0339 - val_acc: 0.8316\n",
            "Epoch 162/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0347 - acc: 0.8231 - val_loss: 0.0337 - val_acc: 0.8322\n",
            "Epoch 163/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0345 - acc: 0.8243 - val_loss: 0.0335 - val_acc: 0.8329\n",
            "Epoch 164/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0343 - acc: 0.8253 - val_loss: 0.0333 - val_acc: 0.8342\n",
            "Epoch 165/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0342 - acc: 0.8265 - val_loss: 0.0332 - val_acc: 0.8351\n",
            "Epoch 166/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0340 - acc: 0.8278 - val_loss: 0.0330 - val_acc: 0.8358\n",
            "Epoch 167/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0338 - acc: 0.8285 - val_loss: 0.0328 - val_acc: 0.8372\n",
            "Epoch 168/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0337 - acc: 0.8297 - val_loss: 0.0327 - val_acc: 0.8382\n",
            "Epoch 169/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0335 - acc: 0.8308 - val_loss: 0.0325 - val_acc: 0.8391\n",
            "Epoch 170/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0333 - acc: 0.8317 - val_loss: 0.0323 - val_acc: 0.8395\n",
            "Epoch 171/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0332 - acc: 0.8326 - val_loss: 0.0322 - val_acc: 0.8402\n",
            "Epoch 172/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0330 - acc: 0.8335 - val_loss: 0.0320 - val_acc: 0.8411\n",
            "Epoch 173/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0329 - acc: 0.8342 - val_loss: 0.0319 - val_acc: 0.8417\n",
            "Epoch 174/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0327 - acc: 0.8351 - val_loss: 0.0317 - val_acc: 0.8422\n",
            "Epoch 175/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0326 - acc: 0.8360 - val_loss: 0.0316 - val_acc: 0.8430\n",
            "Epoch 176/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0324 - acc: 0.8369 - val_loss: 0.0314 - val_acc: 0.8438\n",
            "Epoch 177/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0323 - acc: 0.8378 - val_loss: 0.0313 - val_acc: 0.8446\n",
            "Epoch 178/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0321 - acc: 0.8383 - val_loss: 0.0311 - val_acc: 0.8454\n",
            "Epoch 179/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0320 - acc: 0.8393 - val_loss: 0.0310 - val_acc: 0.8460\n",
            "Epoch 180/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0318 - acc: 0.8400 - val_loss: 0.0308 - val_acc: 0.8472\n",
            "Epoch 181/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0317 - acc: 0.8410 - val_loss: 0.0307 - val_acc: 0.8480\n",
            "Epoch 182/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0315 - acc: 0.8419 - val_loss: 0.0305 - val_acc: 0.8479\n",
            "Epoch 183/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0314 - acc: 0.8425 - val_loss: 0.0304 - val_acc: 0.8485\n",
            "Epoch 184/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0313 - acc: 0.8433 - val_loss: 0.0303 - val_acc: 0.8494\n",
            "Epoch 185/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0311 - acc: 0.8439 - val_loss: 0.0301 - val_acc: 0.8499\n",
            "Epoch 186/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0310 - acc: 0.8446 - val_loss: 0.0300 - val_acc: 0.8504\n",
            "Epoch 187/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0309 - acc: 0.8454 - val_loss: 0.0298 - val_acc: 0.8505\n",
            "Epoch 188/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0307 - acc: 0.8459 - val_loss: 0.0297 - val_acc: 0.8511\n",
            "Epoch 189/200\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0306 - acc: 0.8464 - val_loss: 0.0296 - val_acc: 0.8526\n",
            "Epoch 190/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0305 - acc: 0.8468 - val_loss: 0.0294 - val_acc: 0.8530\n",
            "Epoch 191/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0303 - acc: 0.8474 - val_loss: 0.0293 - val_acc: 0.8537\n",
            "Epoch 192/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0302 - acc: 0.8481 - val_loss: 0.0292 - val_acc: 0.8542\n",
            "Epoch 193/200\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0301 - acc: 0.8489 - val_loss: 0.0291 - val_acc: 0.8550\n",
            "Epoch 194/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0300 - acc: 0.8492 - val_loss: 0.0289 - val_acc: 0.8556\n",
            "Epoch 195/200\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 0.0298 - acc: 0.8498 - val_loss: 0.0288 - val_acc: 0.8561\n",
            "Epoch 196/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0297 - acc: 0.8503 - val_loss: 0.0287 - val_acc: 0.8573\n",
            "Epoch 197/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0296 - acc: 0.8510 - val_loss: 0.0286 - val_acc: 0.8578\n",
            "Epoch 198/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0295 - acc: 0.8518 - val_loss: 0.0284 - val_acc: 0.8583\n",
            "Epoch 199/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0293 - acc: 0.8522 - val_loss: 0.0283 - val_acc: 0.8587\n",
            "Epoch 200/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.0292 - acc: 0.8527 - val_loss: 0.0282 - val_acc: 0.8590\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6bea540748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3aejoICvbn_",
        "colab_type": "code",
        "outputId": "4202c7be-b960-44eb-f597-d7568278de1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.evaluate(X_valid, y_valid)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0282 - acc: 0.8590\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.028211937594413757, 0.859]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMNQVtC3vmuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_0 = X_valid[0].reshape(1, 784)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOBrB-hgvsw0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b9b80f34-0e74-4ba9-ce09-f67348604575"
      },
      "source": [
        "\n",
        "model.predict(valid_0)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.7871342e-03, 1.3557703e-03, 1.0120039e-03, 6.2158001e-03,\n",
              "        6.2417118e-03, 1.0718857e-02, 4.5787048e-04, 9.3212253e-01,\n",
              "        4.7469381e-03, 3.1341344e-02]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CZ2lF51v0vx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d0646a6-d018-42cd-a1f1-fd7194da8c2e"
      },
      "source": [
        "\n",
        "model.predict_classes(valid_0)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    }
  ]
}